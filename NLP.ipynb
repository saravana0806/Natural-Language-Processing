{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13163ccd",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1c0bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c5a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is an Example Text.\"\n",
    "lowercased_text = text.lower()\n",
    "print(lowercased_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c704ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11467716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "\n",
    "cleaned_text = remove_special_characters(\"Hello, world! This is a test.\")\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3523445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "expanded_text = expand_contractions(\"I can't believe it!\")\n",
    "print(expanded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3311fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2036924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "html_text = \"<p>This is <b>HTML</b> content.</p>\"\n",
    "cleaned_text = remove_html_tags(html_text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c8db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0f18c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    return re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "numeric_text = \"There are 123 apples.\"\n",
    "cleaned_text = remove_numbers(numeric_text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1e60f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spell Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de03f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "print(spell('caaar'))  \n",
    "print(spell('mussage')) \n",
    "print(spell('survice')) \n",
    "print(spell('hte'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d29b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6aba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stopwords = [\"this\", \"is\", \"an\",  \"with\", \"some\"]\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Usage:\n",
    "cleaned_text = remove_stopwords(\"This is an example sentence with some stopwords.\")\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2dc562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b503c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_sentences(text):\n",
    "    # Split the text into sentences based on whitespace (space or tab characters)\n",
    "    sentences = text.split()\n",
    "    return sentences\n",
    "\n",
    "input_text = \"This is an example sentence. Another sentence follows.\"\n",
    "sentences = segment_sentences(input_text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413d4f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91d8448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    punctuation_marks = ['.', ',', '!', '?', ';', ':']\n",
    "\n",
    "    cleaned_text = ''.join([char for char in text if char not in punctuation_marks])\n",
    "    return cleaned_text\n",
    "\n",
    "input_text = \"This is an example sentence with some punctuation!\"\n",
    "cleaned_text = remove_punctuation(input_text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d2131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentences = [\n",
    "    \"John likes to watch movies. Mary likes movies too.\",\n",
    "    \"John also likes to watch football games.\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"Sentence {i + 1} vector: {X[i].toarray()[0]}\")\n",
    "\n",
    "print(f\"Vocabulary: {vocabulary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25132bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe435c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sentences = [\n",
    "    \"John likes to watch movies. Mary likes movies too.\",\n",
    "    \"John also likes to watch football games.\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(sentences)\n",
    "\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"Sentence {i + 1} TF-IDF vector: {X_tfidf[i].toarray()[0]}\")\n",
    "\n",
    "print(f\"Vocabulary: {vocabulary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb563c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f250e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b069872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [\n",
    "    \"John likes to watch movies. Mary likes movies too.\".split(),\n",
    "    \"John also likes to watch football games.\".split()\n",
    "]\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Get the vector representation for a specific word (e.g., 'movies')\n",
    "word_vector = model.wv['movies']\n",
    "print(f\"Vector for 'movies': {word_vector}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part-of-Speech Tagging (POS) with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sentence = \"John likes to watch movies. Mary likes movies too.\"\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "print(\"Token\\tPOS Tag\")\n",
    "for token in doc:\n",
    "    print(token.text, \"\\t\", token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad624137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition (NER) with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4a316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sentence = \"Barack Obama was born in Hawaii and served as the 44th President of the United States.\"\n",
    "\n",
    "doc = nlp(sentence)\n",
    "\n",
    "print(\"Named Entities:\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"\\t\", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb4181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ORG: Stands for \"Organization.\" It refers to named entities that represent companies, institutions, or other organized groups.\n",
    "\n",
    "DATE: Represents expressions of dates or periods in time.\n",
    "\n",
    "TIME: Denotes expressions of time, including specific times of the day or durations.\n",
    "\n",
    "MONEY: Indicates expressions of monetary values, including currencies and amounts.\n",
    "\n",
    "PERCENT: Refers to expressions of percentages, such as \"10%\" or \"50 percent.\"\n",
    "\n",
    "CARDINAL: Represents cardinal numbers, which are numerical quantities, including both integers and floating-point numbers.\n",
    "\n",
    "FAC: Stands for \"Facility.\" It refers to named entities that represent buildings, airports, highways, bridges, etc.\n",
    "\n",
    "NORP: Denotes named entities representing nationalities, ethnic groups, or religious groups.\n",
    "\n",
    "PRODUCT: Represents named entities that are products or goods, such as \"iPhone\" or \"Coca-Cola.\"\n",
    "\n",
    "EVENT: Indicates named entities representing events, such as \"World War II\" or \"Super Bowl.\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a949029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a905ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "text = \"I love this product! It's amazing.\"\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentiment_scores = analyzer.polarity_scores(text)\n",
    "\n",
    "print(\"Sentiment Scores:\", sentiment_scores)\n",
    "\n",
    "if sentiment_scores['compound'] >= 0.05:\n",
    "    print(\"Overall Sentiment: Positive\")\n",
    "elif sentiment_scores['compound'] <= -0.05:\n",
    "    print(\"Overall Sentiment: Negative\")\n",
    "else:\n",
    "    print(\"Overall Sentiment: Neutral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65d4ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815924f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data = fetch_20newsgroups(subset='all', categories=None, shuffle=True, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
